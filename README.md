
# Tá»‘i Æ°u Danh má»¥c Äáº§u tÆ° báº±ng Deep Reinforcement Learning (PPO + Transformer)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Pytorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-orange)
![Stable-Baselines3](https://img.shields.io/badge/Stable--Baselines3-RL-green)
![HUTECH](https://img.shields.io/badge/HUTECH-University-red)

## ğŸ“– Giá»›i thiá»‡u

Dá»± Ã¡n nÃ y lÃ  **Äá»“ Ã¡n ChuyÃªn ngÃ nh** ngÃ nh CÃ´ng nghá»‡ ThÃ´ng tin (Há»c mÃ¡y vÃ  á»©ng dá»¥ng) táº¡i trÆ°á»ng Äáº¡i há»c CÃ´ng nghá»‡ TP.HCM (HUTECH).

Má»¥c tiÃªu cá»§a dá»± Ã¡n lÃ  xÃ¢y dá»±ng má»™t há»‡ thá»‘ng tá»‘i Æ°u hÃ³a danh má»¥c Ä‘áº§u tÆ° tá»± Ä‘á»™ng trÃªn thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam (VN30), sá»­ dá»¥ng káº¿t há»£p giá»¯a thuáº­t toÃ¡n **Proximal Policy Optimization (PPO)** vÃ  kiáº¿n trÃºc máº¡ng **Transformer**. MÃ´ hÃ¬nh Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ tá»± Ä‘á»™ng há»c chiáº¿n lÆ°á»£c phÃ¢n bá»• tÃ i sáº£n nháº±m tá»‘i Ä‘a hÃ³a lá»£i nhuáº­n Ä‘Ã£ Ä‘iá»u chá»‰nh rá»§i ro (Risk-adjusted Return) trong bá»‘i cáº£nh thá»‹ trÆ°á»ng phi tuyáº¿n vÃ  biáº¿n Ä‘á»™ng máº¡nh.

**Sinh viÃªn thá»±c hiá»‡n:** Pháº¡m Ngá»c TÃº  
**Giáº£ng viÃªn hÆ°á»›ng dáº«n:** ThS. Nguyá»…n Há»¯u Trung

---

## ğŸš€ TÃ­nh nÄƒng ná»•i báº­t

* **MÃ´ hÃ¬nh cá»‘t lÃµi:** Sá»­ dá»¥ng thuáº­t toÃ¡n PPO (Proximal Policy Optimization) - má»™t trong nhá»¯ng thuáº­t toÃ¡n SOTA cá»§a Reinforcement Learning.
* **Kiáº¿n trÃºc máº¡ng:** TÃ­ch há»£p **Transformer Encoder** lÃ m Policy Network Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng chuá»—i thá»i gian dÃ i háº¡n vÃ  cÃ¡c má»‘i quan há»‡ phi tuyáº¿n giá»¯a cÃ¡c tÃ i sáº£n.
* **Quáº£n trá»‹ rá»§i ro:** HÃ m thÆ°á»Ÿng (Reward Function) Ä‘Æ°á»£c thiáº¿t káº¿ thÃ´ng minh, pháº¡t náº·ng cÃ¡c má»©c sá»¥t giáº£m tÃ i sáº£n (Max Drawdown) Ä‘á»ƒ báº£o vá»‡ vá»‘n.
* **Dá»¯ liá»‡u thá»±c táº¿:** Huáº¥n luyá»‡n vÃ  kiá»ƒm thá»­ trÃªn dá»¯ liá»‡u lá»‹ch sá»­ cá»§a rá»• cá»• phiáº¿u **VN30** (2010 - 2025).
* **Backtesting:** Há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n vá»›i cÃ¡c chá»‰ sá»‘ tÃ i chÃ­nh chuyÃªn sÃ¢u (Sharpe Ratio, Calmar Ratio, MDD).

---

## ğŸ› ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

MÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng dá»±a trÃªn quy trÃ¬nh tÆ°Æ¡ng tÃ¡c giá»¯a Agent vÃ  Environment:

1.  **Input (State):** Tensor 3 chiá»u `(Window Size Ã— Max Assets Ã— Features)` bao gá»“m GiÃ¡ (OHLC), Volume, vÃ  GiÃ¡ trá»‹ giao dá»‹ch trong 30 phiÃªn gáº§n nháº¥t.
2.  **Network:**
    * **Feature Extractor:** Transformer Encoder (Multi-Head Self-Attention) giÃºp náº¯m báº¯t xu hÆ°á»›ng thá»‹ trÆ°á»ng.
    * **Policy Network:** ÄÆ°a ra hÃ nh Ä‘á»™ng lÃ  vector trá»ng sá»‘ danh má»¥c (Portfolio Weights).
    * **Value Network:** Æ¯á»›c lÆ°á»£ng giÃ¡ trá»‹ tráº¡ng thÃ¡i Ä‘á»ƒ tÃ­nh toÃ¡n Advantage (GAE).
3.  **Action:** PhÃ¢n bá»• tá»· trá»ng tÃ i sáº£n (Ä‘Ã£ chuáº©n hÃ³a, khÃ´ng bÃ¡n khá»‘ng).
4.  **Reward:** Log-return cá»§a danh má»¥c trá»« Ä‘i chi phÃ­ giao dá»‹ch vÃ  pháº¡t drawdown.

---

## ğŸ“Š Káº¿t quáº£ thá»±c nghiá»‡m

MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 2 triá»‡u bÆ°á»›c (timesteps) vÃ  kiá»ƒm thá»­ trÃªn táº­p dá»¯ liá»‡u tá»« 01/01/2023 Ä‘áº¿n 10/11/2025. Káº¿t quáº£ cho tháº¥y sá»± vÆ°á»£t trá»™i vá» quáº£n lÃ½ rá»§i ro so vá»›i thá»‹ trÆ°á»ng chung.

### So sÃ¡nh hiá»‡u suáº¥t (Test Set)

| Chá»‰ tiÃªu | AI Agent (PPO + Transformer) | Benchmark (VN30) | Equal-Weight |
| :--- | :---: | :---: | :---: |
| **Lá»£i nhuáº­n nÄƒm (ARR)** | `24.88%` | 29.76% | 25.38% |
| **Max Drawdown (MDD)** | **-11.35%** | -16.22% | -18.09% |
| **Sharpe Ratio** | **1.323** | 1.500 | 1.340 |
| **Calmar Ratio** | **2.193** | 1.835 | 1.403 |

> **Nháº­n xÃ©t:** AI Agent cÃ³ má»©c sá»¥t giáº£m tÃ i sáº£n (Max Drawdown) tháº¥p nháº¥t vÃ  chá»‰ sá»‘ Calmar Ratio cao nháº¥t, cho tháº¥y kháº£ nÄƒng báº£o toÃ n vá»‘n vÃ  hiá»‡u quáº£ Ä‘áº§u tÆ° bá»n vá»¯ng hÆ¡n so vá»›i viá»‡c náº¯m giá»¯ VN30 thá»¥ Ä‘á»™ng.

---

## ğŸ“‚ Cáº¥u trÃºc dá»± Ã¡n


```

â”œâ”€â”€ data/                   # Dá»¯ liá»‡u chá»©ng khoÃ¡n VN30 (Raw & Processed)
â”œâ”€â”€ envs/                   # MÃ´i trÆ°á»ng Gym tá»§y chá»‰nh cho Trading
â”‚   â””â”€â”€ environment.py
â”œâ”€â”€ models/                 # Kiáº¿n trÃºc máº¡ng PPO vÃ  Transformer
â”‚   â””â”€â”€ transformer_policy.py
â”œâ”€â”€ notebooks/              # Jupyter Notebooks phÃ¢n tÃ­ch vÃ  trá»±c quan hÃ³a
â”œâ”€â”€ train.py                # Script huáº¥n luyá»‡n mÃ´ hÃ¬nh
â”œâ”€â”€ backtest.py             # Script kiá»ƒm thá»­ vÃ  Ä‘Ã¡nh giÃ¡
â”œâ”€â”€ requirements.txt        # CÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
â””â”€â”€ README.md

```

---

## âš™ï¸ CÃ i Ä‘áº·t vÃ  Sá»­ dá»¥ng

### 1. YÃªu cáº§u há»‡ thá»‘ng
* Python 3.8 trá»Ÿ lÃªn
* ThÆ° viá»‡n: PyTorch, Stable-Baselines3, Pandas, Numpy, Gymnasium.

### 2. CÃ i Ä‘áº·t

```bash
# Clone repository
git clone [https://github.com/username/portfolio-optimization-drl.git](https://github.com/username/portfolio-optimization-drl.git)
cd portfolio-optimization-drl

# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n phá»¥ thuá»™c
pip install -r requirements.txt

```

### 3. Huáº¥n luyá»‡n mÃ´ hÃ¬nh

Äá»ƒ báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh PPO vá»›i Transformer policy:

```bash
python train.py --timesteps 2000000

```

### 4. Backtest

Äá»ƒ xem káº¿t quáº£ giao dá»‹ch trÃªn táº­p Test:

```bash
python backtest.py --model_path logs/best_model.zip

```

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

Dá»± Ã¡n Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn cÃ¡c nghiÃªn cá»©u ná»n táº£ng:

1. *Proximal Policy Optimization Algorithms* - Schulman et al. (2017).
2. *Attention is All You Need* - Vaswani et al. (2017).
3. *Deep Reinforcement Learning in Finance* - Ye et al. (2020).
4. TÃ i liá»‡u thá»±c hiá»‡n Ä‘á»“ Ã¡n tá»‘t nghiá»‡p cá»§a SV Pháº¡m Ngá»c TÃº - HUTECH (2025).

---

## ğŸ“ LiÃªn há»‡

Má»i tháº¯c máº¯c hoáº·c Ä‘Ã³ng gÃ³p cho dá»± Ã¡n, vui lÃ²ng liÃªn há»‡:

* **Pháº¡m Ngá»c TÃº**
* Email: [Email cá»§a báº¡n]
* GitHub: [Link GitHub cá»§a báº¡n]

### Má»™t sá»‘ lÆ°u Ã½ Ä‘á»ƒ file README Ä‘áº¹p hÆ¡n:

1.  **áº¢nh minh há»a:** Trong bÃ¡o cÃ¡o cá»§a báº¡n cÃ³ **áº¢nh 3.2.1 (Biá»ƒu Ä‘á»“ tÄƒng trÆ°á»Ÿng tÃ i sáº£n)**. Báº¡n nÃªn chá»¥p mÃ n hÃ¬nh biá»ƒu Ä‘á»“ Ä‘Ã³ (file áº£nh), lÆ°u vÃ o thÆ° má»¥c `images/` trong dá»± Ã¡n vÃ  chÃ¨n vÃ o file README (ngay dÆ°á»›i pháº§n Káº¿t quáº£ thá»±c nghiá»‡m) báº±ng cÃº phÃ¡p: `![Káº¿t quáº£ Backtest](images/chart_result.png)`. NÃ³ sáº½ lÃ m dá»± Ã¡n trÃ´ng ráº¥t thuyáº¿t phá»¥c.
2.  **Link Github:** Nhá»› thay tháº¿ cÃ¡c placeholder `username` vÃ  `Link GitHub cá»§a báº¡n` báº±ng Ä‘Æ°á»ng dáº«n tháº­t.
3.  **Email:** Äiá»n email tháº­t náº¿u báº¡n muá»‘n ngÆ°á»i khÃ¡c liÃªn há»‡ (vÃ­ dá»¥ nhÃ  tuyá»ƒn dá»¥ng).

